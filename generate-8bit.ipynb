{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea2a9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_generate(model, prompt: str):\n",
    "        from threading import Thread\n",
    "        from transformers import TextIteratorStreamer\n",
    "        from transformers import GenerationConfig\n",
    "\n",
    "        tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = tokenized.input_ids\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        \n",
    "        tokens_for_summary = 50\n",
    "        output_tokens = input_ids.shape[1] + tokens_for_summary\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            temperature=1.0,\n",
    "            max_new_tokens=output_tokens,\n",
    "        )\n",
    "\n",
    "        streamer = TextIteratorStreamer(\n",
    "            tokenizer, skip_special_tokens=True,\n",
    "        )\n",
    "        generate_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "#             return_dict_in_generate=True,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             bos_token_id=tokenizer.bos_token_id,\n",
    "#             attention_mask=tokenized.attention_mask,\n",
    "#             output_scores=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "        thread = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "        thread.start()\n",
    "        for new_text in streamer:\n",
    "            yield new_text\n",
    "\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(falcon, lora_apply_dir=None, lora_config=None, ddp=None):\n",
    "    if lora_apply_dir is None:\n",
    "        model = get_peft_model(falcon, lora_config)\n",
    "    else:\n",
    "        if ddp:\n",
    "            device_map = {'': 0}\n",
    "        else:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                device_map = \"auto\"\n",
    "            else:\n",
    "                device_map = {'': 0}\n",
    "\n",
    "        print('Device map for lora:', device_map)\n",
    "\n",
    "        model = PeftModel.from_pretrained(\n",
    "            falcon, lora_apply_dir, device_map=device_map,\n",
    "            torch_dtype=torch.float32, is_trainable=True)\n",
    "\n",
    "        print(lora_apply_dir, 'loaded')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "\n",
    "model_id = \"/mnt/artifacts/falcon_40b_model\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Falcon requires you to allow remote code execution. This is because the model uses a new architecture that is not part of transformers yet.\n",
    "# The code is provided by the model authors in the repo.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\", cache_dir='/mnt/artifacts/falcon_40b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Falcon tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model,PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "        ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adapter\n",
    "model.config.use_cache = True\n",
    "# Please check the location and change accordingly. Your checkpoint name may be different\n",
    "model = load_adapter(model, lora_apply_dir='/mnt/artifacts/outputs_sample_8bit/checkpoint-125/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f83471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random test sample\n",
    "sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d12515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Carmen: how are you feeling, Viola? it is so so close...\n",
      "Alfred: My dearest Viola <3\n",
      "Viola: I think as one's feeling before the wedding - a little bit light in the stomach! ive got some things to organize still!\n",
      "Carmen: i will be on friday night, i could give you a helping hand :))\n",
      "Viola: Thanks darling, i will let you know x\n",
      "Carmen: (Y) my number just in case +00123456789\n",
      "Viola: (Y) <3\n",
      "---\n",
      "Summary:\n",
      "1. Carmen asks Viola that how Viola is feeling for her wedding, which is getting closer.\n",
      "\n",
      "I feel i'm not getting the chat dialogue here.\n",
      ">>COMMENT<< @Lampros - I am not sure, if I am getting the same chat dialogue as your answer - could be a bit different.>>COMMENT<< I can understand the chat and my answer matches what you wrote>>COMMENT<< Ohh. That's not the output i received.. Any way, thanks a lot.>>ANSWER<< You did ask a bit different question - you posted a slightly different chat log without any explanation.\n",
      "\n",
      "I assume that the last line should say \"that's the number you have to press (Y)\" or something like that - not as a part of the chat.>>COMMENT<< Yes... I'm really sorry for that. I'll definitely keep that in mind. Thanks for pointing that out for me.>>COMMENT<< no worries,\n"
     ]
    }
   ],
   "source": [
    "#set the tokens for the summary evaluation\n",
    "from transformers import GenerationConfig\n",
    "tokens_for_summary = 50\n",
    "output_tokens = input_ids.shape[1] + tokens_for_summary\n",
    "\n",
    "start_time = time.time()\n",
    "generation_config = GenerationConfig(\n",
    "            do_sample=True,\n",
    "            max_new_tokens=output_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "with torch.no_grad():\n",
    "#     outputs = model.generate(inputs=input_ids, do_sample=True, pad_token_id=tokenizer.pad_token_id, max_length=output_tokens)\n",
    "      outputs = model.generate(inputs=input_ids, generation_config=generation_config)  \n",
    "end_time = time.time()\n",
    "gen_text = tokenizer.batch_decode(outputs)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nTook {round(end_time - start_time, 3)} s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b53aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the output\n",
    "# for text in streaming_generate(model,test_sample):\n",
    "#     print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfc4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
