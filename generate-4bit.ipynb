{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c800a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(falcon, lora_apply_dir=None, lora_config=None, ddp=None):\n",
    "    if lora_apply_dir is None:\n",
    "        model = get_peft_model(falcon, lora_config)\n",
    "    else:\n",
    "        if ddp:\n",
    "            device_map = {'': 0}\n",
    "        else:\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                device_map = \"auto\"\n",
    "            else:\n",
    "                device_map = {'': 0}\n",
    "\n",
    "        print('Device map for lora:', device_map)\n",
    "\n",
    "        model = PeftModel.from_pretrained(\n",
    "            falcon, lora_apply_dir, device_map=device_map,\n",
    "            torch_dtype=torch.float32, is_trainable=True)\n",
    "\n",
    "        print(lora_apply_dir, 'loaded')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Falcon requires you to allow remote code execution. This is because the model uses a new architecture that is not part of transformers yet.\n",
    "# The code is provided by the model authors in the repo.\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\", cache_dir='/mnt/artifacts/falcon_40b/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45016d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Falcon tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ae52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model,PeftModel\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"query_key_value\",\n",
    "        \"dense\",\n",
    "        \"dense_h_to_4h\",\n",
    "        \"dense_4h_to_h\",\n",
    "        ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a670dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adapter\n",
    "model.config.use_cache = True\n",
    "# Please change the path below, it maybe different for your model\n",
    "model = load_adapter(model, lora_apply_dir='/mnt/artifacts/outputs_sample/checkpoint-63/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disc\n",
    "pt_save_directory = \"/mnt/artifacts/fine_tune_model/8bit\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad954c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random test sample\n",
    "sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the tokens for the summary evaluation\n",
    "tokens_for_summary = 50\n",
    "output_tokens = input_ids.shape[1] + tokens_for_summary\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs=input_ids, do_sample=True, pad_token_id=tokenizer.pad_token_id, max_length=output_tokens)\n",
    "end_time = time.time()\n",
    "gen_text = tokenizer.batch_decode(outputs)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b22491",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nTook {round(end_time - start_time, 3)} s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147256b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
